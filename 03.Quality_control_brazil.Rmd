---
title: "Aedes aegypti in Brazil - Quality Control for Brazilian samples"
author: "Luciano V Cosme"
date: "`r Sys.Date()`"
output:
  html_document:
    highlight: breezedark
    css:
      - "styles.css"
    toc: yes
    toc_float: no
    toc_depth: 5
editor_options:
  markdown:
    wrap: 120
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  eval                        = TRUE,
  echo                        = TRUE,
  cache                       = TRUE, # tidy = TRUE,
  class.output                = "bg-success"
)
knitr::opts_knit$set(
  root.dir = rprojroot::find_rstudio_root_file()
)
```
 

<span class="rainbow-title">Analysis code</span>

<!-- Custom JavaScript to apply the rainbow effect to the title -->
<script>
document.addEventListener("DOMContentLoaded", function() {
  var titleElements = document.querySelectorAll('h1');
  if (titleElements.length > 0) {
    titleElements[0].classList.add('rainbow-title');
  }
});
</script>

## 1. Getting started: R libraries and software for QC

### 1.1 R libraries and software

```{r libraries, message=FALSE, results='hide'}
library(tidyverse)
library(here)
library(dplyr)
library(ggplot2)
library(colorout)
library(extrafont)
library(reticulate)
library(scales)
library(stringr)
library(grid)
library(flextable)
library(devtools)
library(readr)
library(purrr)
library(ggtext)
library(ggvenn)
library(data.table)
library(adegenet)
library(dartR)
```

### 1.2 Check directory structure for the project

**Note:** make sure you download the raw data and place it in a new directory. You can check the directory structure
using a tool called [Tree](https://atom.io/packages/ascii-tree "Tool for directory structure.").

```{bash tree, results='hide', eval=FALSE}
# check all the Tree options with the command in your terminal: man tree
tree -L 2 -d --charset=ascii . # -L set the directory depth to show; the flag -d tells tree to return directories only. and the period at the end tells tree to use the current directory; --charset=ascii is to show nicely here.
# if you want to see the files you can use the code below.
# tree --dirsfirst .
```

" .
|-- data
|-- output
|   |-- probes
|   `-- segregation
`-- scripts
    |-- RMarkdown_files
    |-- analysis
    `-- files"

### 1.3 Download software for quality control

I used PLINK v2.00a3.3 64-bit (3 Jun 2022) for all the analysis. However, the Affymetrix software does not export the
files directly into a format we can import into Plink2. Therefore, we need first to use Plink 1.9 to convert the `.ped`
and `.map` file format to `.bed`, `.bin`, and `.fam`. You can install it with `conda`. You can download
[Miniconda](https://conda.io/projects/conda/en/latest/user-guide/install/download.html "To download miniconda") or
[Anaconda](https://www.anaconda.com/ "To download Anaconda."). `conda install -c bioconda plink2`. If you have problems,
you can also get it from [GitHub](https://github.com/chrchang/plink-ng "Download Plink2."), and compile it for your OS
(both versions, 1.9 and 2). Please check [Marees et. al, 2018](https://doi.org/10.1002/mpr.1608 "Link to publication.")
for more information about quality control using Plink 1.9. I used some of their code for quality control, and their
general guidelines.

Check your Plink2 version.
```{bash plink2_version}
plink2 --version
```

**First check if the data is in the correct location:**
```{bash check_files, cache=TRUE}
ls data/brazil** # we use * to truncate the name of the file showing all files
```

Note about the bash multiline comments.<br> Since we will use the Plink command with one option per
line separated by backslash `/` , and not one single line command, we add comments using `` # ` ``
to open and `` ` `` to close.<br> For example, we can split the `plink2 –version` into a multiline
command with a comment.
```{bash commenting_bash_chunks}
plink2 `# this still works` \
--version
```

## 2. The data

I prepared the data with the families and individual ids. I also set the reference alleles to match the AlbF3 genome assembly

We can check how many sample names we have in our vcf
```{bash check_sample_names_in_vcf_file}
# make sure you have all the .CEL samples in your family file
bcftools query -l data/brazil.vcf | wc -l
```
Make sure only biallelic sites are used.
```{bash}
bcftools view -m2 -M2 -v snps data/brazil.vcf > data/brazil2.vcf
```

### 2.1 Use Plink2 to convert to bed format

```{bash plink2_convert_vcf_to_bed1}
# If you run this chunk you will have to open the file.fam in a text editor and set parents id and sex of each individual. I fix it using bash tools. You can start on the next chunk if you do not want to have to repeat what I did.
# we also check if the reference genome and the reference alleles match.
plink2 \
--allow-extra-chr \
--vcf data/brazil2.vcf \
--const-fid \
--make-bed \
--exclude output/segregation/aegypti_SNPs_fail_segregation.txt \
--out output/populations/file1 \
--silent;
grep "variants" output/populations/file1.log; # to get the number of variants from the log file.
```

If you want to get the name of the samples
```{bash}
bcftools query -l data/brazil2.vcf > output/populations/samples_vcf.txt;
head output/populations/samples_vcf.txt
```


Check the fam file
```{bash check_fam_file}
# Check head of data
head output/populations/file1.fam
```


```{bash, cache=TRUE}
head -n 5 data/samples_ped_info.txt
```


### 2.2 Use R to update the .fam file

Import the fam file we use with Axiom Suite

```{r import_fam_file_Axiom, cache=TRUE}
# the order of the rows in this file does not matter
samples <-
  read.delim(
    file   = here(
      "data",
      "samples_ped_info.txt"
    ),
    header = TRUE
  )
head(samples)
```

Import .fam file we created once we created the bed file using Plink2

```{r import_fam_dp}
# The fam file is the same for both data sets with the default or new priors
fam1 <-
  read.delim(
    file   = here(
      "output", "populations", "file1.fam"
    ),
    sep = "\t",
    header = FALSE,
    
  )
head(fam1)
```

We can merge the tibbles.

```{r merge_objects1}
# Create index
fam1_temp <- fam1 |>
  mutate(num_id = seq_len(nrow(fam1)))

# Merge
df <- fam1_temp |>
  left_join(samples, by = c(V2 = "Sample.Filename")) |>
  arrange(num_id) |>
  dplyr::select(8:13)

# check the data frame
head(df)
```

We can check how many samples we have in our file

```{r check_number_samples_df}
nrow(df)
```

Before you save the new fam file, you can change the original file to a different name, to compare the order later. If
you want to repeat the steps above after you saving the new file1.fam, you will need to import the vcf again.

```{r save_new_fam_file}
# Save and override the .fam file for dp
write.table(
  df,
  file      = here(
    "output", "populations", "file1.fam"
  ),
  sep       = "\t",
  row.names = FALSE,
  col.names = FALSE,
  quote     = FALSE
)
```

Check the new .fam file to see if has the order and the sample attributes we want.

```{bash, cache=TRUE}
# you can open the file on a text editor and double check the sample order and information.
head -n 5 output/populations/file1.fam
```

### 2.3 Lift the SNP positions to AaegL5 genome assembly

Check the bim file
```{bash}
head output/populations/file1.bim
```


Import the bim file from the crosses

```{r}
# load the function that we saved earlier
source(
  here(
    "scripts", "analysis", "import_bim.R"
  ),
  local = knitr::knit_global()
)

# import the file
file1 <- import_bim(
  here(
    "output", "populations", "file1.bim"
  )
)

# Convert to data table
setDT(file1)

# Rename columns
setnames(file1, "SNP", "SNP_id")
setnames(file1, "Scaffold", "Scaffold_chip")
setnames(file1, "Position", "Position_chip")

head(file1)
```

Import the file with SNP positions
```{r}
snps_pos <- read.table(
  file      = here("output", "segregation", "snp_pos_AaegL5.txt"),
  sep       = "\t",
  header    = TRUE,
  stringsAsFactors = FALSE
)
# Ensure snps_pos is a data table
setDT(snps_pos)

head(snps_pos)
```

Merge the data
```{r}
# Perform a left join with file1 and snps_pos, by SNP_id
merged_data <- file1 |>
  left_join(snps_pos, by = "SNP_id")

head(merged_data)
```
```{r}
merged_snps <-
  merged_data |>
  dplyr::select(Chromosome, SNP_id, Cm, Position, Allele1, Allele2) |>
  na.omit() 
head(merged_snps)
```

Make sure we remove any duplicated ID (remove both)
```{r}
# Identify duplicate SNP_ids
duplicates <- merged_snps |>
  filter(duplicated(SNP_id) | duplicated(SNP_id, fromLast = TRUE))

# Remove all rows with duplicate SNP_ids
merged_snps_unique <- merged_snps |>
  anti_join(duplicates, by = "SNP_id")

head(merged_snps_unique)
```



Check if the order of the SNP match the file1.bim
```{bash}
head output/populations/file1.bim
```

Before we save the new bim file we need to export the SNP ids that we can use. Remember, not all probe sequences mapped uniquely in the current genome. Then we export the data and extract the SNPs we can use, creating a new bed file.

```{r}
# # Write the unique SNP_ids to a file
write.table(
  merged_snps_unique$SNP_id,
  file      = here("output", "populations", "snp_2_keep.txt"),
  sep       = "\t",
  row.names = FALSE,
  col.names = FALSE,
  quote     = FALSE
)
```

Now use plink to extract the SNPs


```{bash}
plink \
--allow-extra-chr \
-bfile output/populations/file1 \
--make-bed \
--extract output/populations/snp_2_keep.txt \
--out output/populations/file2 \
--silent;
grep "variants" output/populations/file2.log;
```

Save the new .bim file

```{r}
write.table(
  merged_snps_unique,
  file      = here(
    "output", "populations", "file2B.bim"
  ),
  sep       = "\t",
  row.names = FALSE,
  col.names = FALSE,
  quote     = FALSE
)
```

Rename the .bim files

```{bash}
# change the name of the first .bim file, for example, append _backup.bim, and then replace the original file
mv output/populations/file2.bim output/populations/file2_backup.bim;
# than change the new bim we create to the original name (do it only once, otherwise it will mess up)
mv output/populations/file2B.bim output/populations/file2.bim
```


Create a new bed file with Plink to see if it works. For example, to see if the variants are in the
right order

```{bash}
plink \
--bfile output/populations/file2 \
--make-bed \
--out output/populations/file3;
```


Check the new bim file
```{bash}
head output/populations/file3.bim
```

We do not have some of the alleles because we exported all 50k SNPs, and not the recommended SNPs.

Set the reference alleles to match the genome
```{bash}
plink2 \
--bfile output/populations/file3 \
--make-bed \
--fa output/probes/genomes/AaegL5_ncbi.fasta.gz \
--ref-from-fa 'force' `# sets REF alleles when it can be done unambiguously, we use force to change the alleles` \
--out output/populations/file4 \
--silent;
grep "variants" output/populations/file4.log
```


Check the new bim file
```{bash}
head output/populations/file4.bim
```


### 2.2 Checking the number of samples and localities

```{bash get_pops_n, cache=TRUE}
# this part is very important. Make sure there are no NAs or something that is not what you would expect. For example, the number of mosquitoes per population.
awk '{print $1}' output/populations/file4.fam | sort | uniq -c | awk '{print $2, $1}' # here we use awk to print the first column of the file1.fam. Then we sort it using sort, and count the unique occurrences with uniq. The second awk command is for aesthetic reasons, I prefer showing the family name on the left and the counts on the right. You can always check the manual to see explanations for all the tools. For example, type man awk, man sort, or man uniq to see all the operations or options available. The pipe operator (|) passes the output (stdout) of one command as input (stdin) to another. Since we create a command using the pipe operator, we can call it a pipeline.
```

In Linux/Unix we have 3 `I/O streams`: Standard input (`stdin`) - this is the file handle that your process reads to get
information from you.<br> Standard output (`stdout`) - your process writes conventional output to this file handle.<br>
Standard error (`sterr`) - your process writes diagnostic output to this file handle.<br> Most programs need to read
input, write output, and log errors, so `stdin`, `stdout`, and `stderr` are predefined as a programming convenience.<br>


## 3. Quality control steps

### 3.1 Missingness

```{bash plink2_filter_SNP}
plink2 \
--allow-extra-chr \
--bfile output/populations/file4 \
--geno 0.1                `# we set genotype missiningness to 10% with this option` \
--make-bed \
--out output/populations/file5 \
--silent \
--missing;                 # --missing produces sample-based and variant-based missing data reports. If run with --within/--family, the variant-based report is stratified by cluster.
grep "variants" output/populations/file5.log
```

Make plot
```{r plot_ind_missingness, cache=TRUE}
#   ____________________________________________________________________________
#   import individual missingness                                           ####
indmiss <-                                              # name of the data frame we are creating
  read.delim(                                           # use function read table
    file   = here(
      "output", "populations", "file5.smiss"
    ),                                                  # we use library here to import file5.imiss from data/QC
    header = TRUE                                       # we do have headers in our file
  )
#   ____________________________________________________________________________
#   import SNP missingness                                                  ####
snpmiss <-
  read.delim(
    file   = here(
      "output", "populations", "file5.vmiss"
    ),
    header = TRUE
  )
#
```

Plot individual missingness
```{r plot_individual_missingness, cache=TRUE}
# This code uses the ggplot2 package to create a histogram of the variable F_MISS from the dataset indmiss. The code customizes several aspects of the plot to improve its clarity and visual appeal.
#
# The geom_histogram function specifies the color and fill of the bars, as well as the number of bins to use. The stat_bin function adds text labels to each bin indicating the number of observations in that bin. The geom_vline function adds a vertical dotted line at the position of the mean of F_MISS, while geom_text adds a text label indicating the value of the mean as a percentage.
#
# The labs function is used to label the x and y axes of the plot, while theme_minimal customizes the font and size of the plot elements. The scale_x_continuous function modifies the x-axis to display percentages, and the theme function further modifies the appearance of the grid lines and axis labels.
#
# Overall, this code provides a clear and visually appealing representation of the distribution of F_MISS in the indmiss dataset.

# load plotting theme
source(
  here(
    "scripts", "analysis", "my_theme2.R"
  )
)

ggplot( # Start a ggplot object with the data and aesthetic mappings
  indmiss,
  aes(
    x = F_MISS
  )
) +
  geom_histogram( # Add a histogram layer
    color            = "black",
    fill             = "#B6FAD7",
    bins             = 6
  ) +
  geom_text(
    # Add text labels for bin counts
    stat             = "bin",
    aes(
    label = after_stat(count)
  ),
    vjust            = -0.5,
    color            = "purple",
    size             = 3,
    bins             = 6
  ) +
  geom_vline(
    # Add a vertical line at the mean of F_MISS
    aes(
    xintercept = mean(F_MISS)),
    color            = "orange",
    linetype         = "dotted",
    linewidth        = .5
  ) +
  geom_text(
    # Add a text label for the mean of F_MISS
    aes(
      x = mean(F_MISS),
      y = 75,
      label = paste0(
        "Mean \n",
        scales::percent(mean(F_MISS),
          accuracy = 0.01
        )
      )
    ),
    size = 3,
    color = "orange",
    hjust = -.1
  ) +
  labs( # Add axis labels
    x                = "Individual Missingness (%)",
    y                = "Frequency (n)"
  ) +
  my_theme() +
  scale_x_continuous( # Scale the x-axis to display percentages
    labels           = scales::percent,
    n.breaks         = 6
  )
#
# save the plot
ggsave(
  here(
    "output", "populations", "figures" , "individual_missingness.pdf"
  ),
  width              = 7,
  height             = 5,
  units              = "in"
)
```

Plot variant missingness
```{r plot_variant_missingness, cache=TRUE}
# This plot takes a while to compute
# This code creates a histogram from the indmiss data set using the F_MISS column.
# ggplot builds a histogram of individual missingness data
ggplot(
  snpmiss,
  aes(
    x = F_MISS
  )
) +
  geom_histogram(
    color = "black",
    fill = "#B6FAD7",
    bins = 6
  ) +
  stat_bin(
    geom = "text",
    aes(
      label = format(
        after_stat(count),
        big.mark = ",",
        scientific = FALSE
      )
    ),
    vjust = -0.5,
    color = "purple",
    size = 2,
    bins = 6
  ) +
  geom_vline(
    aes(
      xintercept = mean(F_MISS)
    ),
    color = "orange",
    linetype = "dotted",
    linewidth = 0.5
  ) +
  geom_text(
    aes(
      x = mean(F_MISS),
      y = 16000,
      label = paste0(
        "Mean \n",
        scales::percent(mean(F_MISS),
          accuracy = 0.01
        )
      )
    ),
    size = 3,
    color = "orange",
    # hjust = 1.5,
    vjust = -.2
  ) +
  labs(
    x = "Variant Missingness (%)",
    y = "Frequency (n)"
  ) +
  # theme_minimal(
  #   base_size = 12,
  #   base_family = "Roboto Condensed"
  # ) +
  scale_x_continuous(
    labels = scales::percent,
    n.breaks = 6
  ) +
  scale_y_continuous(
    labels = scales::label_comma(),
    n.breaks = 5
  ) +
  my_theme()

# save the plot
ggsave(
  here(
    "output", "populations", "figures", "SNPs_missingness.pdf"
  ),
  width  = 7,
  height = 5,
  units  = "in"
)
```

Remove individuals missing more than 20% of SNPs. You can use the threshold you want, change the flag --mind

```{bash plink2_filter_ind}
plink2 \
--allow-extra-chr \
--bfile output/populations/file5 \
--mind 0.2               `# here we set the individual missingness threshold of 20%`\
--make-bed \
--out output/populations/file6 \
--silent;
grep "samples\|variants" output/populations/file6.log
```
We did not remove any SNP due to individual missingness

### 3.2 Minor allele frequency

First lets make a plot of the MAF. First, we estimate the allele frequency with Plink.
```{bash plink2_MAF_check, eval=FALSE}
plink2 \
--allow-extra-chr \
--bfile output/populations/file6 \
--freq \
--out output/populations/MAF_check \
--silent
```

Then we plot it with ggplot.

```{r import_MAF}
#   ____________________________________________________________________________
#   Import MAF data                                                         ####
maf_freq <-
  read.delim(
    here(
      "output", "populations", "MAF_check.afreq"
    ),
    header = TRUE
  )
```

Make MAF plot
```{r plot_MAF}
#   ____________________________________________________________________________
#   make the plot                                                           ####
ggplot(
  maf_freq,
  aes(ALT_FREQS)
) +
  geom_histogram(
    colour = "black",
    fill = "#C4F3F5",
    bins = 40
  ) +
  labs(
    x = "Minor Allele Frequency (MAF)",
    y = "Frequency (n)",
    caption = "<span style='color:red;'><i>Red</i></span> <span style='color:black;'><i>line at</i></span><span style='color:red;'><i> MAF 10%</i></span><span style='color:black;'><i> threshold</i></span>."
  ) +
  geom_text(
    aes(
      x = .1,
      y = 8000,
      label = paste0("11,314 SNPs")
    ),
    size = 3,
    color = "blue",
    vjust = -.2
  ) +
  geom_vline(xintercept = 0.1, color = "red") +
  my_theme() +
  theme(plot.caption = element_markdown()) +
  scale_y_continuous(label = scales::number_format(big.mark = ",")) +
  scale_x_continuous(breaks = c(0, 0.1, 0.2, 0.4, 0.6, 0.8, 1))
#   ____________________________________________________________________________
#   save the plot                                                           ####
ggsave(
  here(
    "output", "populations", "figures", "MAF.pdf"
  ),
  width  = 5,
  height = 4,
  units  = "in"
)
```

Now we apply the MAF filter.
```{bash filter_MAF, cache=TRUE, eval=FALSE}
# We will use MAF of 5%
plink2 \
--allow-extra-chr \
--bfile output/populations/file6 \
--maf 0.05 \
--make-bed \
--out output/populations/file7 \
--silent;
grep "variants" output/populations/file7.log
```

We removed 11864 variants due to the MAF filter. Next we will excludes markers which deviate from
Hardy--Weinberg equilibrium (HWE). It is a common indicator of genotyping error, but may also
indicate evolutionary selection. We have to do it for each population individually. We cannot do it
for all populations at once. Therefore, the first step is create a new bed file with Plink keeping
only one population. I like to create a new directory and name it "hardy", and copy the "file4"
there.

```{bash mkdirs_4_HWE, eval=FALSE}
mkdir -p output/populations/hardy;
cp output/populations/file7.* output/populations/hardy/
```

### 3.3 HWE test

Now we can run the HWE test. However, we will need to apply the SNP missingness again for each
population. If we do not, the HWE will vary widely. With the bash script below, we will create a new
file for each population, run the HWE test with HWE p value \<1e‐6 (HWE p value \<1e‐6). Then, we
ask Plink to generate a list of SNPs that passed the test for each population.

```{bash loop_filter_HWE, eval=FALSE}
for fam in $(awk '{print $1}' output/populations/hardy/file7.fam | sort | uniq); 
do 
echo $fam | \
plink2 \
--allow-extra-chr \
--silent \
--keep-allele-order \
--bfile output/populations/hardy/file7 \
--keep-fam /dev/stdin \
--make-bed \
--out output/populations/hardy/$fam \
--hwe 0.000001 \
--geno 0.1 \
--write-snplist; \
done
```

Next, we use "cat" and "`awk`" to concatenate the SNP list from all populations, and remove
duplicates. Once we have a list of SNPs that passed the test for each population, we can use Plink
to create a new bed file keeping only the SNPs that passed the test in each population.
First, lets get the list of SNPs, and count how many passed:

```{bash get_SNP_list_after_HWE, eval=FALSE}
cat output/populations/hardy/*.snplist | awk '!a[$0]++' > output/populations/passed_hwe.txt;
wc -l output/populations/passed_hwe.txt
```

How many variants we had before

```{bash check_n_variants_before_HWE}
cat output/populations/file4.bim | awk '{print $2}'| awk '!a[$0]++' | wc -l
```

Variants not passing HWE test
```{r}
13278 - 13278
```
All variants passed HWE test. If some failed, next time we could remove the variants that did not pass HWE test, using the --extract flag, extracting only those that passed HWE.


### 3.4 LD pruning

Since we do not have to remove any SNP due to deviation from HWE, we can proceed with heterozygosity
estimates. The first step is to "prune" our data set. We will check the pairwise linkage estimates
for all SNPs. We can work with file4. We will use "`indep-pairwide`" to check if there are SNPs
above a certain linkage disequilibrium (LD) threshold. Check Plink documentation for more details
<https://www.cog-genomics.org/plink/1.9/ld> I used "`--indep-pairwise 5 1 0.1`" , which indicates
according to the documentation: `--indep-pairphase <window size>['kb'] <step size (variant ct)> <r^2
threshold>` We will check in a window of 5kb if there is any pair of SNPs with r2 estimates above
0.1, then we will move our window 1 SNP and check again for SNPs above the threshold. We will repeat
this procedure until we check the entire genome.

```{bash linkage_prunning, cache=TRUE, eval=FALSE}
# you can change the values of indep-pairwise to see how many more variants are pruned. Ideally, we would use linkage network analysis to remove variants based on the interspersed or mosaic like block distribution.
# we set a window of variants of 5 and move the window 1 variant per time, removing 1 of the variants with lowest MAF from a pair above the threshold of r^2 > 0.1
# Try --indep-pairwise 50kb 1 0.1 to see.
plink2 \
--bfile output/populations/file7 \
--extract output/populations/passed_hwe.txt \
--indep-pairwise 50kb 1 0.1 \
--out output/populations/indepSNP \
--silent;
grep 'pairwise\|variants\|samples' output/populations/indepSNP.log
```

Remember, the SNPs are not removed from our data set. Plink created 3 files when we run the code
above. One is the "indepSNP.log" file, and the other two are:<br>"**indepSNP.prune.in**" -\> list of
SNPs with squared correlation smaller than our r2 threshold of 0.1.<br>"i**ndepSNP.prune.out**" -\>
list of SNPs with squared correlation greater than our r2 threshold of 0.1. For our heterozygosity
estimates, we want to use the set of SNPs that are below our r2 threshold of 0.1. We consider that
they are randomly associated. We can use Plink to estimate the heterozygosity using the
"indepSNP.prune.in" file.

### 3.5 Heterozygosity
```{bash plink1.9_estimate_heterozygosity, cache=TRUE}
plink2 \
--allow-extra-chr \
--bfile output/populations/file7 \
--extract output/populations/indepSNP.prune.in \
--het \
--out output/populations/R_check \
--silent;
grep 'variants' output/populations/R_check.log
```

We can see that we started with 92,693 SNPs, then we only extract those that are not "linked" from
the "indepSNP.prune.in" file. We used these SNPs to estimate heterozygosity. Now we can use R to
part the R_check.het, to find the individuals with excess heterozygosity. We will remove any
individual that deviates more the 3 standard deviations from the mean heterozygosity of the data
set. The code below will create a list of individuals with excess heterozygosity (file named
"`fail-het-qc.txt`"), and make a heterozygosity plot for the entire data set.

```{r plot_heterozygosity, cache=TRUE}
#   ____________________________________________________________________________
#   find individuals with high heterozygosity                              ####
# import the data from Plink
het <- read.delim(
  here(
    "output", "populations", "R_check.het"
  ),
  head = TRUE
)
#
# check head of the file
colnames(het)
```

Estimate het
```{r calculate_het, cache=TRUE}
# create a column named HET_RATE and calculate the heterozygosity rate
het$HET_RATE <- (het$"OBS_CT" - het$"O.HOM") / het$"OBS_CT"
#
# use subset function to get values deviating from 4sd of the mean heterozygosity rate.
het_fail <-
  subset(
    het, (het$HET_RATE < mean(
      het$HET_RATE
    ) - 4 * sd(
      het$HET_RATE
    )) |
      (het$HET_RATE > mean(
        het$HET_RATE
      ) + 4 * sd(
        het$HET_RATE
      ))
  )
#
# get the list of individuals that failed our threshold of 4sd from the mean.
het_fail$HET_DST <-
  (het_fail$HET_RATE - mean(
    het$HET_RATE
  )) / sd(
    het$HET_RATE
  )
```

Save the files to use with Plink

```{r save_list_fail_het, cache=TRUE}
#   ____________________________________________________________________________
#   save the data to use with Plink2                                        ####
#
write.table(
  het_fail,
  here(
    "output", "populations", "fail-het-qc.txt"
  ),
  row.names = FALSE
)
```

Make plot
```{r plot_het, cache=TRUE}
#   ____________________________________________________________________________
#   make a heterozygosity plot                                              ####
#
ggplot(
  het,
  aes(
    HET_RATE
  )
) +
  geom_histogram(
    colour           = "black",
    fill             = "#CDFAF8",
    bins             = 40
  ) +
  labs(
    x                = "Heterozygosity Rate",
    y                = "Number of Individuals"
  ) +
  geom_vline(
    aes(
      xintercept     = mean(
        HET_RATE
      )
    ),
    col              = "#F2C46F",
    linewidth        = 1.5
  ) +
  geom_vline(
    aes(
      xintercept     = mean(
        HET_RATE
      ) + 4 * sd(
        HET_RATE
      )
    ),
    col              = "#BFB9B9",
    linewidth        = 1
  ) +
  geom_vline(
    aes(
      xintercept     = mean(
        HET_RATE
      ) - 4 * sd(
        HET_RATE
      )
    ),
    col              = "#BFB9B9",
    linewidth        = 1
  ) + 
  my_theme() +
  scale_y_continuous(
    labels           = comma
  )
#   ____________________________________________________________________________
#   save the heterozygosity plot                                            ####
ggsave(
  here(
    "output", "populations", "figures", "Heterozygosity.pdf"
  ),
  width  = 5,
  height = 4,
  units  = "in"
)
```

The red line in the plot above indicates the mean, and the orange line indicate 4 standard deviation
from the mean. We can see that some mosquitoes do have excess heterozygous sites. We will remove
them. We can get their ID from the file "`fail-het-qc.txt`". We can use the bash script below to
parse the file to use with Plink

```{bash parse_R_het_output, cache=TRUE}
sed 's/"// g' output/populations/fail-het-qc.txt | awk '{print$1, $2}'> output/populations/het_fail_ind.txt;
echo 'How many mosquitoes we need to remove from our data set:';
cat output/populations/het_fail_ind.txt | tail -n +2 | wc -l;
echo 'Which mosquitoes we have to remove:';
tail -n +2 output/populations/het_fail_ind.txt
```
The population from Nepal has high heterozygosity reate. We will remove 4 individuals from this population and one from QNC

Next, we will remove these mosquitoes from our data set using Plink:
```{bash plink2_remove_fail_het, cache=TRUE, eval=FALSE}
plink2 \
--allow-extra-chr \
--bfile output/populations/file7 \
--remove output/populations/het_fail_ind.txt \
--make-bed \
--out output/populations/file8 \
--silent;
grep 'variants\|samples' output/populations/file8.log
```

### 3.6 Relatedness

Check for cryptic relatedness. Check Plink2 documentation
<https://www.cog-genomics.org/plink/2.0/distance> You can download King directly
<https://www.kingrelatedness.com/manual.shtml> or check their manuscript
<https://www.ncbi.nlm.populations.gov/pmc/articles/PMC3025716/pdf/btq559.pdf>
From Plink2 documentation: "Note that KING kinship coefficients are scaled such that duplicate
samples have kinship 0.5, not 1. First-degree relations (parent-child, full siblings) correspond to
\~0.25, second-degree relations correspond to \~0.125, etc. It is conventional to use a cutoff of
\~0.354 (the geometric mean of 0.5 and 0.25) to screen for monozygotic twins and duplicate samples,
\~0.177 to add first-degree relations, etc." There two options. One is to run only --make-king and
another one is to use --make-king-table We will use the threshold of 0.354 and create a table.


```{bash plink2_make_king, cache=TRUE}
# Plink2 will create a file with extension .king
plink2 \
--allow-extra-chr \
--bfile output/populations/file8 \
--extract output/populations/indepSNP.prune.in \
--make-king-table rel-check \
--king-table-filter 0.354 \
--out output/populations/file9 \
--silent;
grep 'variants\|samples' output/populations/file9.log
```

Check the individuals that did not pass our filtering.

```{bash check_kin0, cache=TRUE}
head output/populations/file9.kin0
```

We want to remove one of the individuals of the pairs.
```{bash remove_related1}
plink2 \
--allow-extra-chr \
--bfile output/populations/file8 \
--extract output/populations/indepSNP.prune.in \
--make-king triangle bin \
--out output/populations/file9 \
--silent;
grep 'variants\|samples' output/populations/file9.log
```

Now we can use Plink2 to remove one of the mosquitoes from the pair with high kinship. It will
remove 32 samples since we had 2 samples in some populations with high relatedness, and we could
remove 1 and keep the other two. Plink2 always tries to maximize the number of samples passing the
filters.
```{bash}
plink2 \
--allow-extra-chr \
--bfile output/populations/file8 \
--king-cutoff output/populations/file9 0.354 \
--make-bed \
--extract output/populations/indepSNP.prune.in \
--out output/populations/file10 \
--silent;
grep 'samples\|variants\|remaining' output/populations/file10.log
```

Now we can check the individuals that were removed.
```{bash check_id_removed_individuals}
# we have 19 samples that were removed, and our file has heading, so we need to see 13 lines of the file
head -n 10 output/populations/file10.king.cutoff.out.id
```

### 3.7 Quick PCA with Plink using the LD pruned data after removing the related individuals.
We will run PCA analysis with LEA latter, but we can get a quick PCA using Plink.

Create list of families to exclude from previous study
```{bash}
echo "CGO
IGU
ITB
ITP
MAG
PQM
PQN
PQS
VAS" > output/populations/families_2_remove.txt
```

PCA
```{bash plink_pca}
plink2 \
--allow-extra-chr \
--remove-fam output/populations/families_2_remove.txt \
--bfile output/populations/file10 \
--pca allele-wts \
--freq \
--out output/populations/pca_pops \
--silent;
grep 'samples\|variants' output/populations/pca_pops.log
```

Check the files
```{bash check_eigenvec}
head -n 2 output/populations/pca_pops.eigenvec
```

```{bash check_eigenval}
head -n 2 output/populations/pca_pops.eigenval
```

Import PCA data
```{r import_pca}
# import the data from Plink
pca <- read.delim(
  here(
    "output", "populations", "pca_pops.eigenvec"
  ),
  head = TRUE
)
 
# check head of the file
head(pca)
```

Create sample attributes
```{r}
cities <- tibble::tribble(
   ~pop,                  ~city, ~state, ~latitude, ~longitude, ~region,           ~biome,
  "BRA",             "Brasilia",   "DF", -15.7939,  -47.8827,  "Central-West",    "Cerrado",
  "GOI",              "Goiania",   "GO", -16.6864,  -49.2643,  "Central-West",    "Cerrado",
  "CAM",         "Campo Grande",   "MS", -20.4428,  -54.6460,  "Central-West",    "Pantanal",
  "CUI",               "Cuiaba",   "MT", -15.6014,  -56.0974,  "Central-West",    "Cerrado",
  "RBR",           "Rio Branco",   "AC",  -9.9746,  -67.8180,  "North",           "Amazon",
  "MAN",               "Manaus",   "AM",  -3.1316,  -59.9825,  "North",           "Amazon",
  "MAC",               "Macapa",   "AP",   0.0356,  -51.0705,  "North",           "Amazon",
  "BEL",                "Belem",   "PA",  -1.4554,  -48.4902,  "North",           "Amazon",
  "MAR",               "Maraba",   "PA",  -5.3811,  -49.1326,  "North",           "Amazon",
  "FOZ",           "Foz Iguacu",   "PR", -25.5478,  -54.5881,  "South",           "Atlantic Forest",
  "POR",          "Porto Velho",   "RO",  -8.7642,  -63.9039,  "North",           "Amazon",
  "BOA",            "Boa Vista",   "RR",   2.8197,  -60.6733,  "North",           "Amazon",
  "PAL",               "Palmas",   "TO", -10.2491,  -48.3243,  "North",           "Cerrado",
  "MAE",               "Maceio",   "AL",  -9.6658,  -35.7353,  "Northeast",       "Atlantic Forest",
  "IRE",                "Irece",   "BA", -11.3042,  -41.8555,  "Northeast",       "Caatinga",
  "SAL",             "Salvador",   "BA", -12.9747,  -38.4767,  "Northeast",       "Atlantic Forest",
  "FOR",            "Fortaleza",   "CE",  -3.7305,  -38.5218,  "Northeast",       "Caatinga",
  "SLU",             "Sao Luis",   "MA",  -2.5391,  -44.2829,  "Northeast",       "Amazon",
  "BHO",       "Belo Horizonte",   "MG", -19.9227,  -43.9451,  "Southeast",       "Cerrado",
  "JOA",          "Joao Pessoa",   "PB",  -7.1195,  -34.8450,  "Northeast",       "Atlantic Forest",
  "REC",               "Recife",   "PE",  -8.0642,  -34.8782,  "Northeast",       "Atlantic Forest",
  "PAR",             "Parnaiba",   "PI",  -2.9056,  -41.7769,  "Northeast",       "Caatinga",
  "TER",             "Teresina",   "PI",  -5.0892,  -42.8096,  "Northeast",       "Caatinga",
  "MOS",              "Mossoro",   "RN",  -5.1831,  -37.3472,  "Northeast",       "Caatinga",
  "NAT",                "Natal",   "RN",  -5.7945,  -35.2110,  "Northeast",       "Atlantic Forest",
  "ARA",              "Aracaju",   "SE", -10.9091,  -37.0675,  "Northeast",       "Atlantic Forest",
  "SAO",            "Sao Paulo",   "SP", -23.5505,  -46.6333,  "Southeast",       "Atlantic Forest",
  "QUA",               "Quarai",   "RS", -30.3826,  -56.4483,  "South",           "Pampa",
  "CHA",              "Chapeco",   "SC", -27.0966,  -52.6182,  "South",           "Atlantic Forest",
  "ITA",               "Itajai",   "SC", -26.9106,  -48.6706,  "South",           "Atlantic Forest",
  "VIT",              "Vitoria",   "ES", -20.2976,  -40.2958,  "Southeast",       "Atlantic Forest",
  "UBE",              "Uberaba",   "MG", -19.7472,  -47.9381,  "Southeast",       "Cerrado",
  "CGO", "Campos de Goytacazes",   "RJ", -21.7622,  -41.3246,  "Southeast",       "Atlantic Forest",
  "IGU",        "Iguaba Grande",   "RJ", -22.8495,  -42.2293,  "Southeast",       "Atlantic Forest",
  "ITB",             "Itaborai",   "RJ", -22.7580,  -42.8687,  "Southeast",       "Atlantic Forest",
  "ITP",            "Itaperuna",   "RJ", -21.1997,  -41.8799,  "Southeast",       "Atlantic Forest",
  "MAG",          "Mangaratiba",   "RJ", -22.9594,  -44.0409,  "Southeast",       "Atlantic Forest",
  "PQM",              "Paqueta",   "RJ", -22.7592,  -43.108989,  "Southeast",       "Atlantic Forest",
  "PQN",              "Paqueta",   "RJ", -22.750868,  -43.106185,  "Southeast",       "Atlantic Forest",
  "PQS",              "Paqueta",   "RJ", -22.768324,  -43.109541,  "Southeast",       "Atlantic Forest",
  "RIO",          "Rio Janeiro",   "RJ", -22.9068,  -43.1729,  "Southeast",       "Atlantic Forest",
  "VAS",            "Vassouras",   "RJ", -22.4059,  -43.6689,  "Southeast",       "Atlantic Forest",
  "RIB",       "Ribeirao Preto",   "SP", -21.1775,  -47.8102,  "Southeast",       "Cerrado"
)

head(cities)
```

Save it
```{r}
saveRDS(cities, here("output", "populations", "cities.rds"))
```


Check number of samples per population
```{r pca_sample_count}
pops_pca <- 
  pca |>
  group_by(X.FID) |>
  summarize(count_distinct = n_distinct(IID))

# check it
head(pops_pca)
```

Merge the data
```{r merge_samples_pca}
df4 <-
  merge(
    pca,
    cities,
    by.x = 1,
    by.y = 1,
    all.x = T,
    all.y = F
  ) |>
  na.omit()
head(df4)
```

Get some data for the PCA plot
```{r}
# How many samples
length(unique(df4$IID))
# How many populations
length(unique(df4$X.FID))
# How many regions
length(unique(df4$region))
# How many biomes
length(unique(df4$biome))
# How many states
length(unique(df4$state))
```

Check the regions
```{r}
unique(df4$region)
```

Check the biomes
```{r}
unique(df4$biome)
```


Make plot showing biomes
```{r}
# source the plotting function
source(
  here(
    "scripts", "analysis", "my_theme2.R"
  )
)

colorblind_palette <-
  c(
    "#DE84E1",
    "#E1AF84",
    "#87E184",
    "#84B6E1",
    "#424354",
    "#A90026"
  )


# make plot by continent and range
ggplot(df4, aes(PC1, PC2)) +
  geom_point(aes(fill = biome, shape = region), colour = "gray") +
  stat_ellipse(aes(fill = biome, group = biome), geom = "polygon", alpha = 0.2, level = 0.8) +
  xlab("PC1 (15.74% Variance)") +
  ylab("PC2 (11.88% Variance)") +
  labs(caption = "Principal Component Analysis using 8,481 unlinked SNPs \n from 374 mosquitoes from 34 localities across all state in Brazil.") +
  guides(
    fill = guide_legend(
      title = "Biome",
      order = 2,
      override.aes = list(shape = 21),
      ncol = 1
    ),
    shape = guide_legend(title = "Region", order = 1)
  ) +
  scale_fill_manual(values = colorblind_palette) + # Use colorblind_palette
  scale_shape_manual(values = c(21, 22, 23, 24, 25)) + # Use shapes that support both fill and colour
  my_theme() +
  theme(
    plot.caption = element_text(face = "italic"),
    legend.position = "right",
    legend.justification = "top",
    # set the legend to the top
    legend.box.just = "center",
    legend.box.background = element_blank(),
    # remove the legend border
    plot.margin = margin(5.5, 30, 5.5, 5.5, "points"),
    # increase right margin
    legend.margin = margin(10, 10, 10, 10) # move the legend a bit up
  )

# #   ____________________________________________________________________________
# #   save the pca plot                                                       ####
ggsave(
  here(
    "output", "populations", "figures", "PCA_pc1_2_plink_biome.pdf"
  ),
  width  = 10,
  height = 6,
  units  = "in"
)
```

Make plot showing regions
```{r}
# source the plotting function
source(
  here(
    "scripts", "analysis", "my_theme2.R"
  )
)

colorblind_palette <-
  c(
    "#DE84E1",
    "#E1AF84",
    "#87E184",
    "#84B6E1",
    "#424354",
    "#A90026"
  )

# make plot by continent and range
ggplot(df4, aes(PC1, PC2)) +
  geom_point(aes(fill = region, shape = biome ), colour = "gray") +
  stat_ellipse(aes(fill = region, group = region), geom = "polygon", alpha = 0.2, level = 0.8) +
  xlab("PC1 (15.74% Variance)") +
  ylab("PC2 (11.88% Variance)") +
  labs(caption = "Principal Component Analysis using 8,481 unlinked SNPs \n from 374 mosquitoes from 34 localities across all state in Brazil.") +
  guides(
    fill = guide_legend(
      title = "Region",
      order = 2,
      override.aes = list(shape = 21),
      ncol = 1
    ),
    shape = guide_legend(title = "Biome", order = 1)
  ) +
  scale_fill_manual(values = colorblind_palette) + # Use colorblind_palette
  scale_shape_manual(values = c(21, 22, 23, 24, 25, 20)) + # Use shapes that support both fill and colour
  my_theme() +
  theme(
    plot.caption = element_text(face = "italic"),
    legend.position = "right",
    legend.justification = "top",
    # set the legend to the top
    legend.box.just = "center",
    legend.box.background = element_blank(),
    # remove the legend border
    plot.margin = margin(5.5, 30, 5.5, 5.5, "points"),
    # increase right margin
    legend.margin = margin(10, 10, 10, 10) # move the legend a bit up
  )

# #   ____________________________________________________________________________
# #   save the pca plot                                                       ####
ggsave(
  here(
    "output", "populations", "figures", "PCA_pc1_2_plink_regions.pdf"
  ),
  width  = 10,
  height = 6,
  units  = "in"
)
```
### 3.8 PCA with regions without overlap

For the local adaptation analysis we will analyse the 3 regions that do not overlap first to get familiar with the data. Then, we will use the entire data set to run the analysis.

We can select the samples from North, South and Northeast

North
```{r}
pops_north <- df4 |>
  dplyr::filter(
    region == "North"
  ) |>
  distinct(X.FID)
pops_north
```

Save it
```{r}
write.table(
  pops_north,
  here(
    "output", "populations", "pops_north.txt"
  ),
  row.names = FALSE,
  quote = FALSE,
  col.names = FALSE
)
```

Northeast
```{r}
pops_northeast <- df4 |>
  dplyr::filter(
    region == "Northeast"
  ) |>
  distinct(X.FID)
pops_northeast
```

Save it
```{r}
write.table(
  pops_northeast,
  here(
    "output", "populations", "pops_northeast.txt"
  ),
  row.names = FALSE,
  quote = FALSE,
  col.names = FALSE
)
```

South
```{r}
pops_south <- df4 |>
  dplyr::filter(
    region == "South"
  ) |>
  distinct(X.FID)
pops_south
```

Save it
```{r}
write.table(
  pops_south,
  here(
    "output", "populations", "pops_south.txt"
  ),
  row.names = FALSE,
  quote = FALSE,
  col.names = FALSE
)
```

All 3 regions
```{r}
# We will remove samples from cerrado because they overlap with North and South
pops_regions <- df4 |>
  dplyr::filter(
    region %in% c("North", "South", "Northeast") & biome != "Cerrado"
  ) |>
  distinct(X.FID)

pops_regions
```

Save it
```{r}
write.table(
  pops_regions,
  here(
    "output", "populations", "pops_regions.txt"
  ),
  row.names = FALSE,
  quote = FALSE,
  col.names = FALSE
)
```


PCA with only the 3 regions
PCA
```{bash}
plink2 \
--allow-extra-chr \
--keep-fam output/populations/pops_regions.txt \
--bfile output/populations/file10 \
--pca allele-wts \
--freq \
--out output/populations/pca_regions \
--silent;
grep 'samples\|variants' output/populations/pca_regions.log
```

Check eigenvalues
```{bash}
head -n 2 output/populations/pca_regions.eigenval
```

Import PCA data
```{r}
# import the data from Plink
pca_regions <- read.delim(
  here(
    "output", "populations", "pca_regions.eigenvec"
  ),
  head = TRUE
)
 
# check head of the file
head(pca_regions)
```

Merge the data
```{r}
df5 <-
  merge(
    pca_regions,
    cities,
    by.x = 1,
    by.y = 1,
    all.x = T,
    all.y = F
  ) |>
  na.omit()
head(df5)
```

Make plot showing 3 regions
```{r}
# source the plotting function
source(
  here(
    "scripts", "analysis", "my_theme2.R"
  )
)

colorblind_palette <-
  c(
    "#DE84E1",
    "#E1AF84",
    "#87E184",
    "#84B6E1",
    "#424354",
    "#A90026"
  )



# make plot by continent and range
ggplot(df5, aes(PC1, PC2)) +
  geom_point(aes(fill = region, shape = biome ), colour = "gray") +
  stat_ellipse(aes(fill = region, group = region), geom = "polygon", alpha = 0.2, level = 0.8) +
  xlab("PC1 (13.81% Variance)") +
  ylab("PC2 (10.13% Variance)") +
  labs(caption = "Principal Component Analysis using 8,481 unlinked SNPs \n from 247 mosquitoes from 3 regions in Brazil.") +
  guides(
    fill = guide_legend(
      title = "Region",
      order = 2,
      override.aes = list(shape = 21),
      ncol = 1
    ),
    shape = guide_legend(title = "Biome", order = 1)
  ) +
  scale_fill_manual(values = colorblind_palette) + # Use colorblind_palette
  scale_shape_manual(values = c(21, 22, 23, 24, 25, 20)) + # Use shapes that support both fill and colour
  my_theme() +
  theme(
    plot.caption = element_text(face = "italic"),
    legend.position = "right",
    legend.justification = "top",
    # set the legend to the top
    legend.box.just = "center",
    legend.box.background = element_blank(),
    # remove the legend border
    plot.margin = margin(5.5, 30, 5.5, 5.5, "points"),
    # increase right margin
    legend.margin = margin(10, 10, 10, 10) # move the legend a bit up
  )

# #   ____________________________________________________________________________
# #   save the pca plot                                                       ####
ggsave(
  here(
    "output", "populations", "figures", "PCA_pc1_2_plink_3_regions.pdf"
  ),
  width  = 10,
  height = 6,
  units  = "in"
)
```


### 3.9 PCA with biomes without overlap

Amazon
```{r}
pops_amazon <- df4 |>
  dplyr::filter(
    biome == "Amazon"
  ) |>
  distinct(X.FID)
pops_amazon
```

Save it
```{r}
write.table(
  pops_amazon,
  here(
    "output", "populations", "pops_amazon.txt"
  ),
  row.names = FALSE,
  quote = FALSE,
  col.names = FALSE
)
```

Caatinga
```{r}
pops_caatinga <- df4 |>
  dplyr::filter(
    biome == "Caatinga"
  ) |>
  distinct(X.FID)
pops_caatinga
```

Save it
```{r}
write.table(
  pops_caatinga,
  here(
    "output", "populations", "pops_caatinga.txt"
  ),
  row.names = FALSE,
  quote = FALSE,
  col.names = FALSE
)
```

Pampa
```{r}
pops_pampa <- df4 |>
  dplyr::filter(
    biome == "Pampa"
  ) |>
  distinct(X.FID)
pops_pampa
```

Save it
```{r}
write.table(
  pops_pampa,
  here(
    "output", "populations", "pops_pampa.txt"
  ),
  row.names = FALSE,
  quote = FALSE,
  col.names = FALSE
)
```

All 3 biomes
```{r}
pops_biomes <- df4 |>
  dplyr::filter(
    biome %in% c("Amazon", "Caatinga", "Pampa")
  ) |>
  distinct(X.FID)

pops_biomes
```

Save it
```{r}
write.table(
  pops_biomes,
  here(
    "output", "populations", "pops_biomes.txt"
  ),
  row.names = FALSE,
  quote = FALSE,
  col.names = FALSE
)
```

PCA with only the 3 biomes
PCA
```{bash}
plink2 \
--allow-extra-chr \
--keep-fam output/populations/pops_biomes.txt \
--bfile output/populations/file10 \
--pca allele-wts \
--freq \
--out output/populations/pca_biomes \
--silent;
grep 'samples\|variants' output/populations/pca_biomes.log
```

Check eigenvalues
```{bash}
head -n 2 output/populations/pca_biomes.eigenval
```

Import PCA data
```{r}
# import the data from Plink
pca_biomes <- read.delim(
  here(
    "output", "populations", "pca_biomes.eigenvec"
  ),
  head = TRUE
)
 
# check head of the file
head(pca_biomes)
```

Merge the data
```{r}
df6 <-
  merge(
    pca_biomes,
    cities,
    by.x = 1,
    by.y = 1,
    all.x = T,
    all.y = F
  ) |>
  na.omit()
head(df6)
```

Make plot showing 3 biomes
```{r}
# source the plotting function
source(
  here(
    "scripts", "analysis", "my_theme2.R"
  )
)

colorblind_palette <-
  c(
    "#DE84E1",
    "#E1AF84",
    "#87E184",
    "#84B6E1",
    "#424354",
    "#A90026"
  )


# make plot by continent and range
ggplot(df6, aes(PC1, PC2)) +
  geom_point(aes(fill = biome, shape = region), colour = "gray") +
  stat_ellipse(aes(fill = biome, group = biome), geom = "polygon", alpha = 0.2, level = 0.8) +
  xlab("PC1 (9.75% Variance)") +
  ylab("PC2 (7.52% Variance)") +
  labs(caption = "Principal Component Analysis using 8,481 unlinked SNPs \n from 148 mosquitoes from 3 biomes in Brazil.") +
  guides(
    fill = guide_legend(
      title = "Biome",
      order = 2,
      override.aes = list(shape = 21),
      ncol = 1
    ),
    shape = guide_legend(title = "Region", order = 1)
  ) +
  scale_fill_manual(values = colorblind_palette) + # Use colorblind_palette
  scale_shape_manual(values = c(21, 22, 23, 24, 25)) + # Use shapes that support both fill and colour
  my_theme() +
  theme(
    plot.caption = element_text(face = "italic"),
    legend.position = "right",
    legend.justification = "top",
    # set the legend to the top
    legend.box.just = "center",
    legend.box.background = element_blank(),
    # remove the legend border
    plot.margin = margin(5.5, 30, 5.5, 5.5, "points"),
    # increase right margin
    legend.margin = margin(10, 10, 10, 10) # move the legend a bit up
  )

# #   ____________________________________________________________________________
# #   save the pca plot                                                       ####
ggsave(
  here(
    "output", "populations", "figures", "PCA_pc1_2_plink_3_biomes.pdf"
  ),
  width  = 10,
  height = 6,
  units  = "in"
)
```


## 4. Plot SNP density

After quality control and pruning

```{r import_qc_data_for_density_plot, warning=FALSE}
# load the function that we saved earlier
source(
  here(
    "scripts", "analysis", "import_bim.R"
  ),
  local = knitr::knit_global()
)
# import the file
snp_den_qc <- import_bim(
  here(
    "output", "populations", "file10.bim"
  )
)
```

Make plot of the SNP density

```{r make_density_plot_qc_data, cache=TRUE}
#   ____________________________________________________________________________
#   plot SNP density after QC                                               ####
snp_den_qc |>
  rename(
    Chromosome = 1
  ) |>
  mutate(
    Position           = as.numeric(
      Position
    )
  ) |>
  ggplot(
    aes(
      x                = Position
    ),
    label              = sprintf(
      "%0.2f",
      round(
        a,
        digits         = 0
      )
    )
  ) +
  geom_histogram(
    aes(
      y                = after_stat(
        count
      )
    ),
    binwidth           = 1e6
  ) +
  facet_wrap(
    vars(
      Chromosome
    ),
    scales             = "free_x"
  ) +
  labs(
    title              = "SNP Density after QC and LD pruning",
    x                  = expression(
      "Position in the genome (Mb)"
    ),
    y                  = expression(
      "Number of SNPs"
    )
  ) +
  scale_x_continuous(
    labels             = function(x) {
      format(
        x / 1e6,
        big.mark       = ",", 
        scientific     = FALSE
      )
    }
  ) +
  geom_density(
  aes(
    y = 1e6 * after_stat(count)
  ),
  color = "red",
  linewidth = .75,
  alpha = .4,
  fill = "pink"
  ) +
  hrbrthemes::theme_ipsum(
    base_family        = "Roboto Condensed",
    axis_text_size     = 12,
    axis_title_size    = 14,
    plot_margin        = margin(
      10, 10, 10, 10
    ),
    grid               = TRUE,
    grid_col           = "#fabbe2"
  ) +
  theme(
    panel.grid.major   = element_line(
      linetype         = "dashed",
      linewidth        = 0.2
    ),
    panel.grid.minor   = element_line(
      linetype         = "dashed",
      linewidth        = 0.2
    ),
    panel.spacing      = unit(0.5, "lines"),
    strip.text         = element_text(
      face             = "bold", hjust = .5
    ),
    strip.background.x = element_rect(
      color            = "gray"
    )
  )
#   ____________________________________________________________________________
#   save the density plot                                            ####
ggsave(
  here(
    "output", "populations","figures", "snp_density_after_qc.pdf"
  ),
  width  = 10,
  height = 6,
  units  = "in"
)
```

SNPs per chromosome

```{r SNPs_per_chromsomosome_after_QC}
# we can use dplyr "count" to get the number of SNPs for each chromosome
# lets get the data we need
snps_per_chrm <- 
  snp_den_qc |>
  count(
    Scaffold) |>
  rename(
    Chromosome = 1,
    "SNPs (N) " = 2
  )

# Create the flextable
ft <- flextable::flextable(snps_per_chrm)

# Apply zebra theme
ft <- flextable::theme_zebra(ft)

# Add a caption to the table
ft <- flextable::add_header_lines(ft, "SNPs per chromosome after quality control")
ft
```

We can get the mean number of SNPs per chromosome or the entire genome

```{r SNPs_per_1Mb_window}
# we first use dplyr cut_width to get the number of SNPs per 1Mb window
albo_den <- 
  snp_den_qc |>
  dplyr::select(
    Scaffold, Position
  ) |>
  group_by(
    Scaffold,
    windows               = cut_width(
      Position,
      width               = 1e6,
      boundary            = 0
    )
  ) |>
  summarise(
    n                     = n(),
    .groups               = "keep"
  ) |>
  group_by(
    Scaffold
  ) |>
  summarise(
    mean                  = mean(n),
    n                     = n(),
    .groups               = "keep"
  ) |>
  rename(
    Chromosome            = 1,
    "SNPs per 1Mb window" = 2,
    "Number of windows"   = 3
  )
#
# check the results
snp_table <-
  flextable(
    albo_den
  )
snp_table <- colformat_double(
  x        = snp_table,
  big.mark = ",",
  digits   = 2,
  na_str   = "N/A"
)
snp_table
```

Merge objects

```{r merge_obj}
# we can merge the two data sets we created above into one table
after_qc <-
  snps_per_chrm |>
  left_join(
    albo_den,
    by = "Chromosome"
  )
snp_table2 <- flextable(
  after_qc)
snp_table2 <- colformat_double(
  x        = snp_table2,
  big.mark = ",",
  digits   = 2, 
  na_str   = "N/A"
  )
snp_table2
```

## 5. Subset populations to use only new populations

We genotyped samples collected in 2016 and 2018. The samples from 2016 are from the Rio de Janeiro State only. We can remove them and create new files with only the samples collected later. We also need to remove the samples from Paqueta, 2019.

Check the populations
```{bash, cache=TRUE}
awk '{print $1}' output/populations/file10.fam | sort | uniq -c | awk '{print $2, $1}'
```

Create list of families to exclude
```{bash}
echo "CGO
IGU
ITB
ITP
MAG
PQM
PQN
PQS
VAS" > output/populations/families_2_remove.txt
```


Subset
```{bash}
plink2 \
--bfile output/populations/file10 \
--remove-fam output/populations/families_2_remove.txt \
--make-bed \
--export vcf \
--maf 0.05 \
--geno 0.2 \
--out output/populations/snps_sets/brazil_2018 \
--silent;
grep 'samples\|variants\|remaining' output/populations/snps_sets/brazil_2018.log
```

Create file for variant annotation
We want to annotate all variants that passed our quality control before LD pruning.

We do not remove linked SNPs
```{bash}
plink2 \
--allow-extra-chr \
--bfile output/populations/file8 \
--king-cutoff output/populations/file9 0.354 \
--make-bed \
--out output/populations/file11 \
--silent;
grep 'samples\|variants\|remaining' output/populations/file11.log
```

Create vcf removing populations previous publications
```{bash snpeff}
plink2 \
--bfile output/populations/file11 \
--remove-fam output/populations/families_2_remove.txt \
--export vcf \
--make-bed \
--maf 0.05 \
--geno 0.2 \
--out output/populations/snps_sets/snpeff \
--silent;
grep 'samples\|variants\|remaining' output/populations/snps_sets/snpeff.log
```


# 6. PCA with DAPC

```{r}
# Remove all objects from the environment
rm(list = ls())

# Run the garbage collector to free up memory
gc()
```

```{r, cache=TRUE}
# the order of the rows in this file does not matter
samples <-
  read.delim(
    file   = here(
      "data",
      "samples_ped_info.txt"
    ),
    header = TRUE
  )
head(samples)
```

Create files
```{bash}
plink \
--keep-allele-order \
--bfile output/populations/snps_sets/brazil_2018 \
--make-bed \
--out output/populations/dapc1 \
--silent;
grep 'samples\|variants\|remaining' output/populations/dapc1.log
```

Check fam file
```{bash}
head output/populations/dapc1.fam
```

Import .fam file we created once we created the bed file using Plink2

```{r}
fam_file_path <- here("output", "populations", "dapc1.fam")
fam1 <- read.table(fam_file_path, header = FALSE)

head(fam1)
```

We can merge the tibbles.

```{r}
# Extract the number part from the columns
fam1_temp <- fam1 |>
  mutate(num_id = as.numeric(str_extract(V2, "^\\d+")))

# Assuming your data frame is named fam1
fam1$V2 <- paste(fam1$V1, fam1$V2, sep = "_")

# To check the first few rows of the modified data frame
head(fam1)
```
Save it
```{r}
# Save and override the .fam file for dp
write.table(
  fam1,
  file      = here(
    "output", "populations", "dapc1.fam"
  ),
  sep       = "\t",
  row.names = FALSE,
  col.names = FALSE,
  quote     = FALSE
)
```

Check the new .fam file to see if has the order and the sample attributes we want.

```{bash, cache=TRUE}
# you can open the file on a text editor and double check the sample order and information.
head -n 5 output/populations/dapc1.fam
```

```{bash}
plink \
--allow-extra-chr \
--keep-allele-order \
--bfile output/populations/dapc1 \
--recodeA \
--out output/populations/dapc2 \
--silent;
grep 'samples\|variants\|remaining' output/populations/dapc2.log
```


```{r, eval=FALSE}
### DAPC (in adegenet) #####
snp <- 
  read.PLINK(
    here(
      "output", "populations", "dapc2.raw"
    ),
    quiet = FALSE,
    chunkSize = 1000,
    parallel = require("parallel"),
    n.cores = 4
  )

nInd(snp)
nLoc(snp)
nPop(snp)
indNames(snp)
```
Save it
```{r, eval=FALSE}
saveRDS(
  snp, here(
    "output", "populations", "snp.rds"
  )
)
```

To load it
```{r}
snp <- readRDS(
  here(
    "output", "populations", "snp.rds"
  )
)
```

```{r}
# Check for duplicates
duplicates <- duplicated(indNames(snp))

# See if there are any duplicates
any(duplicates)
```
If there is any, what is it
```{r}
# Find duplicates - both first occurrence and later duplicates
duplicates <- indNames(snp)[duplicated(indNames(snp)) | duplicated(indNames(snp), fromLast = TRUE)]

# Print the duplicate IDs
unique(duplicates)
```

Convert to genid
```{r, eval=FALSE}
snp2 <- gl2gi(snp, probar = FALSE, verbose = NULL)
```

Scale
```{r, eval=FALSE}
snp3 <- scaleGen(snp2, NA.method="mean")
class(snp3)
```
Save it
```{r, eval=FALSE}
saveRDS(
  snp3, here(
    "output", "populations", "snp3.rds"
  )
)
```

To load it
```{r}
snp3 <- readRDS(
  here(
    "output", "populations", "snp3.rds"
  )
)
```


```{r}
dim(snp3)
snp3[1:5,1:5]
```

```{r}
# Get the populations from the genlight object
populations <- snp$pop
```


Run DAPC with  object
```{r, eval=FALSE}
dapc_snp <- dapc(snp3, n.pca = 7, n.da = 8, grp = populations)
```

Save it
```{r, eval=FALSE}
saveRDS(
  dapc_snp, here(
    "output", "populations", "dapc_snp.rds"
  )
)
```

To load it
```{r}
dapc_snp <- readRDS(
  here(
    "output", "populations", "dapc_snp.rds"
  )
)
```

```{r}
library(RColorBrewer)

# Define a base palette
base_palette <- brewer.pal(9, "Set1")

# Interpolate to create a larger palette
color_palette <- colorRampPalette(base_palette)(34)

pop_colors <- setNames(color_palette, unique(snp@pop))
```

Plot using different discriminant functions
```{r, fig.height=10, fig.width=10}
# 1 and 2
scatter(
  dapc_snp,
  bg = "white",
  scree.da = TRUE,
  cex = 1,
  pch = 20,
  clabel=0.7,
  cex.lab = 0.1,
  col = pop_colors,
  xax = 1, 
  yax = 2  
)

```
```{r, fig.height=10, fig.width=10}
scatter(
  dapc_snp,
  bg = "white",
  scree.da = TRUE,
  cex = 1,
  pch = 20,
  clabel=0.7,
  cex.lab = 0.1,
  col = pop_colors,
  xax = 1, 
  yax = 3
)
```

```{r, fig.height=10, fig.width=10}
scatter(
  dapc_snp,
  bg = "white",
  scree.da = TRUE,
  cex = 1,
  pch = 20,
  clabel=0.7,
  cex.lab = 0.1,
  col = pop_colors,
  xax = 1, 
  yax = 4
)
```

```{r}
pdf(here(
    "output", "populations", "figures" , "PCA_plot_discriminat_functions_12.pdf"), width = 6, height = 6)
col <- funky(34)
scatter(
  dapc_snp,
  bg = "white",
  scree.da = TRUE,
  cex = 1,
  pch = 20,
  clabel=0.7,
  cex.lab = 0.1,
  col = pop_colors,
  xax = 1, 
  yax = 2
)
dev.off()
```


```{r}
pdf(here(
    "output", "populations", "figures" , "PCA_plot_discriminat_functions_13.pdf"), width = 6, height = 6)
col <- funky(34)
scatter(
  dapc_snp,
  bg = "white",
  scree.da = TRUE,
  cex = 1,
  pch = 20,
  clabel=0.7,
  cex.lab = 0.1,
  col = pop_colors,
  xax = 1, 
  yax = 3
)
dev.off()
```
```{r}
pdf(here(
    "output", "populations", "figures" , "PCA_plot_discriminat_functions_14.pdf"), width = 6, height = 6)
col <- funky(34)
scatter(
  dapc_snp,
  bg = "white",
  scree.da = TRUE,
  cex = 1,
  pch = 20,
  clabel=0.7,
  cex.lab = 0.1,
  col = pop_colors,
  xax = 1, 
  yax = 4
)
dev.off()
```
```{r}
pdf(here(
    "output", "populations", "figures" , "PCA_plot_discriminat_functions_23.pdf"), width = 6, height = 6)
col <- funky(34)
scatter(
  dapc_snp,
  bg = "white",
  scree.da = TRUE,
  cex = 1,
  pch = 20,
  clabel=0.7,
  cex.lab = 0.1,
  col = pop_colors,
  xax = 2, 
  yax = 3
)
dev.off()
```

```{r}
pca1 <- dudi.pca(snp3,cent=FALSE,scale=FALSE,scannf=FALSE,nf=8)
barplot(pca1$eig[1:50],main="PCA eigenvalues", col=heat.colors(50))
```


```{r}
pca1
```

```{r, fig.height=10, fig.width=10}
col <- funky(34)
# s.class(pca1$li, pop(snp),xax=1,yax=3, col=transp(col,.6), axesell=FALSE,
#         cstar=0, cpoint=2, grid=FALSE)

s.class(pca1$li, col=transp(col,.6), pop(snp))
title("Axes 1-2")
add.scatter.eig(pca1$eig[1:20], 3,1,2)
```


```{r, fig.height=10, fig.width=10}
col <- funky(34)
s.class(pca1$li,pop(snp),xax=1,yax=3,sub="PCA 1-3",csub=2, col=transp(col,.6)) 
title("Axes 1-3")
add.scatter.eig(pca1$eig[1:20],nf=3,xax=1,yax=3)
```

PC 1 and 2
```{r, fig.height=10, fig.width=10}
pdf(here(
    "output", "populations", "figures" , "PCA_plot_axes_1_2.pdf"), width = 8, height = 8)
col <- funky(34)
s.class(pca1$li,pop(snp),xax=1,yax=2,csub=2, col=transp(col,.6),grid=FALSE)
title("Axes 1-2")
add.scatter.eig(pca1$eig[1:20],nf=8,xax=1,yax=2)
dev.off()
```


PC 1 and 3
```{r, fig.height=10, fig.width=10}
pdf(here(
    "output", "populations", "figures" , "PCA_plot_axes_1_3.pdf"), width = 8, height = 8)
col <- funky(34)
s.class(pca1$li,pop(snp),xax=1,yax=3,csub=2, clabel=0.7, col=transp(col,.6),grid=FALSE)
title("Axes 1-3")
add.scatter.eig(pca1$eig[1:20],nf=8,xax=1,yax=3)
dev.off()
```

PC 1 and 4
```{r, fig.height=10, fig.width=10}
pdf(here(
    "output", "populations", "figures" , "PCA_plot_axes_1_4.pdf"), width = 8, height = 8)
col <- funky(34)
s.class(pca1$li,pop(snp),xax=1,yax=4,csub=2, col=transp(col,.6),grid=FALSE)
title("Axes 1-4")
add.scatter.eig(pca1$eig[1:20],nf=8,xax=1,yax=4)
dev.off()
```
PC 2 and 4
```{r, fig.height=10, fig.width=10}
pdf(here(
    "output", "populations", "figures" , "PCA_plot_axes_2_4.pdf"), width = 8, height = 8)
col <- funky(34)
s.class(pca1$li,pop(snp),xax=2,yax=4,csub=2, col=transp(col,.6),grid=FALSE)
title("Axes 2-4")
add.scatter.eig(pca1$eig[1:20],nf=8,xax=2,yax=4)
dev.off()
```
PC 2 and 3
```{r, fig.height=10, fig.width=10}
pdf(here(
    "output", "populations", "figures" , "PCA_plot_axes_2_3.pdf"), width = 8, height = 8)
col <- funky(34)
s.class(pca1$li,pop(snp),xax=2,yax=3,csub=2, col=transp(col,.6),grid=FALSE)
title("Axes 2-3")
add.scatter.eig(pca1$eig[1:20],nf=8,xax=2,yax=3)
dev.off()
```

PC 1 and 4
```{r, fig.height=10, fig.width=10}
pdf(here(
    "output", "populations", "figures" , "PCA_plot_axes_1_4.pdf"), width = 8, height = 8)
col <- funky(34)
s.class(pca1$li,pop(snp),xax=1,yax=4,csub=2, col=transp(col,.6),grid=FALSE)
title("Axes 1-4")
add.scatter.eig(pca1$eig[1:20],nf=8,xax=1,yax=4)
dev.off()
```


PC 3 and 4
```{r, fig.height=10, fig.width=10}
pdf(here(
    "output", "populations", "figures" , "PCA_plot_axes_3_4.pdf"), width = 8, height = 8)
col <- funky(34)
s.class(pca1$li,pop(snp),xax=3,yax=4,csub=2, col=transp(col,.6),grid=FALSE)
title("Axes 3-4")
add.scatter.eig(pca1$eig[1:20],nf=8,xax=3,yax=4)
dev.off()
```






